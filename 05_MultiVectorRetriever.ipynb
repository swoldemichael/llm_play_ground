{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcae4df-ffa0-4adb-a65b-49b673e02db4",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. ) https://huggingface.co/docs/huggingface_hub/package_reference/inference_client\n",
    "2. ) https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\n",
    "3. ) https://blog.langchain.dev/semi-structured-multi-modal-rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84e67d-d48a-4791-8863-b8359e1d0c56",
   "metadata": {},
   "source": [
    "### Get a free Hugging Face Account and API Access Key\n",
    "### I used Amazon SageMaker Studio to create. You can get a free account. I am also using a free Google Colab account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ce343-a6a5-4825-a6c9-4ae139b56f97",
   "metadata": {},
   "source": [
    "### Install libraries to use langchain and hugging face embeddings and LLMs - Use free HuggingFaceEmbeddings and LLMs. I found these useful for learning and experimenting.\n",
    "\n",
    "### Reference above use OpenAI models. The motivation for this notebook is to show how to use Hugging Face Hub hosted free models through LangChain integrations (which I have also locally updated and will hopefuly contribute to LangChain if changes are ccepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee566e84-81ad-4a66-968b-c12595fbf1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet python_dotenv langchain huggingface_hub  sentence_transformers chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e7b3e-d35b-4365-8993-d6d83e89b042",
   "metadata": {},
   "source": [
    "### First thing is first: import you HUGGINGFACE_API_TOKEN which is your HF access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c01565f-58de-4e34-97dd-f13ba15c7dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hugging_face_access_token = os.environ['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddca188-16f0-459e-9e34-38a8a4b3d215",
   "metadata": {},
   "source": [
    "### Instatiate a free HuggingFace Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a44ce4-aae2-43ab-a13f-ccf62e81c328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2846a58a-bc78-4487-b8f0-fcda4c0e2260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a42e3c-0772-4d8a-8f07-51bb98658ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,\n",
       " [-0.04895173758268356,\n",
       "  -0.03986186906695366,\n",
       "  -0.021562783047556877,\n",
       "  0.009908422827720642,\n",
       "  -0.038103993982076645])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = huggingface_embeddings .embed_query(text)\n",
    "len(query_result), query_result[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b705cd4-9412-46c8-87ee-833081977a1f",
   "metadata": {},
   "source": [
    "### MultiVector retrievers 1) with parent docs (InMemoryByteStore), 2) with smaller chunks, 3) with document summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161e6efb-e1c2-4b1d-b835-c20f6d6d8df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da4268-e9fb-4511-a69b-f9f83196d561",
   "metadata": {},
   "source": [
    "## Process Parent Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e92f07-18e4-4be5-8caf-6c137ecdac03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add more text loaders as required \n",
    "loaders = [\n",
    "    TextLoader(\"../state_of_the_union.txt\"),\n",
    "    \n",
    "]\n",
    "# larger documents to be used in InMemoryStore\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "len(docs), len(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fe8a5-c7d1-4df9-a4c8-7b723821142e",
   "metadata": {},
   "source": [
    "## Process Smaller Chunks\n",
    "### Use case - retrieve larger chunks of information, but embed smaller chunks to capture the semantic meaning as closely as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08718a0-f713-4da8-a9c1-2fc7369b60d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "id_key = \"doc_id\"\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4cf3a-3c07-4ee1-a00a-c59068dd8687",
   "metadata": {},
   "source": [
    "### Create a MultiVectorRetriever with vectore store for smaller chunks and for larger documents use InMemoryByte store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22cf4d3f-9c73-465b-b17c-4d712211dc90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents_chunks\", embedding_function=huggingface_embeddings\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "# Retriever was empty to begin with, add smaller chunks in the vector store\n",
    "# add the larger docs in the InMemoryByte store\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2adc312-652e-4a5f-a350-a5f4c26d2aac",
   "metadata": {},
   "source": [
    "#### Vectorstore alone retrieves the small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9238c534-1660-4731-aca9-34911c14d003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '88a9d717-b506-49ee-a4e9-11d84e56c4c6', 'source': '../state_of_the_union.txt'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908bf4d-bd12-4cc6-8361-b96fa437af01",
   "metadata": {},
   "source": [
    "### Retriever returns larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0980433d-8eda-4188-8dea-4a903ce9a2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.get_relevant_documents(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab23f0-f9ce-493c-af39-daec52f6f9cf",
   "metadata": {},
   "source": [
    "## Summaries\n",
    "### Use case - a summary may be able to distill more accurately what a chunk is about, leading to better retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b211d74a-bc89-4f97-817b-ff456a363096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import llm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688c030-cfb3-4ba4-90d5-1092f5bb348c",
   "metadata": {},
   "source": [
    "### Instatiate a HuggingFace LLM for free using your access toke above and create a langchain expression chain to process summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426439e8-6b0a-47c9-a509-61c2c87dcdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a free llm for text summarization from Hagging Hub. This model is currently available through an\n",
    "# llms_util local library. Will propose updating the one that is available in langchain mainly because Hugging Face Hub\n",
    "# is now using InferenceClient and depracating the use of InferenceAPI which LangChain integration uses\n",
    "llm = llm_utils.HuggingFaceHub(task=\"summarization\")\n",
    "\n",
    "# create and use a langchain expression language chain\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a747eb9-4d42-4b6b-8e0a-38f2a45cbf36",
   "metadata": {},
   "source": [
    "## Process Summaries for parent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935b6674-408c-4be9-8d91-c45895d696c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch process with concurrency\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8108c8f-b8aa-4abc-b1dc-72cd576da501",
   "metadata": {},
   "source": [
    "### Create a MultiVectorRetriever with the summary vector store and parent docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0cc6b1-90be-412e-af40-d47ef8d3e9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " ' Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways . But he badly miscalculated . Instead he met a wall of strength he never imagined . Putin is now isolated from the world more than ever. We are inflicting pain on Russia and supporting the people of Ukraine .')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries), summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e71a438-ca69-44b7-a24d-40ae0cbbfb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summary chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=huggingface_embeddings)\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ec0e9-d7ae-466d-995c-7266f64955a6",
   "metadata": {},
   "source": [
    "#### Retrieve from summaries vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ccfce46-ff59-428e-9867-7c338c5ce084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\" The Bipartisan Infrastructure Law is the most sweeping investment to rebuild America in history . America used to have the best roads, bridges, and airports on Earth . Now our infrastructure is ranked 13th in the world. We won't be able to compete for the jobs of the 21st Century if we don’t fix that. We’ll create good jobs for millions of Americans .\", metadata={'doc_id': 'f89e9a68-e1c8-4d0f-bba9-7b31ce466866'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_results = vectorstore.similarity_search(\"justice breyer\")\n",
    "summary_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc851f-aaef-4d8b-b8b1-95254e40166c",
   "metadata": {},
   "source": [
    "#### Retrieve from document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61208620-6a55-4ffc-9bbd-607096d2ad8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9902"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"justice breyer\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6965852-9983-4a81-ab4e-3f731ea40c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
