{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4aqdjk9m22K"
   },
   "source": [
    "### Reference:\n",
    "\n",
    "1 ) https://python.langchain.com/docs/integrations/llms/huggingface_hub\n",
    "\n",
    "2 ) https://python.langchain.com/docs/integrations/chat/huggingface\n",
    "\n",
    "### This notebook is to demostrate:\n",
    "\n",
    "1 ) How to work with tools in langchain\n",
    "\n",
    "2 ) How to create your own tools\n",
    "\n",
    "3 ) How to use a HuggingFace Hub llm\n",
    "\n",
    "4 ) How to create a ChatHaggingFace wrapper - chat model\n",
    "\n",
    "5 ) How to create the chat agent\n",
    "\n",
    "6 ) How to create an Agent Executer with your tools and chat model\n",
    "\n",
    "7 ) How to set observation as a chat model stop condition\n",
    "\n",
    "8 ) How to invoke an Agent Executor and get results in verbose mode - try different prompts and see if it works\n",
    "\n",
    "Note: HuggingFaceHub and ChatHaggingFace are updated locally and are imported via llm_utils library\n",
    "\n",
    "This notebook is created in Amazon SageMaker Studio Lab - free account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_XPPkf4L4fc"
   },
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NnnBwvlnHJS",
    "outputId": "82013302-bc63-4000-f604-368de6ba04ea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet python_dotenv langchain pip install langchainhub huggingface_hub  text-generation transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAsmq1o4ZB4i",
    "outputId": "b979da42-4929-4048-c86b-6a9adc854e67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/studio-lab-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from huggingface_hub import login\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hugging_face_access_token = os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
    "login(hugging_face_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import llm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5VzzN3AxrUd",
    "outputId": "19582500-f7dc-441a-9d6b-d770d0ca36d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = llm_utils.HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7EsBehPmuh37",
    "outputId": "ffdf8bda-ebda-44c3-cabd-c0da7e103426",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' about a fish\\n\\nthat will make me laugh out loud'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note you can override model paramaters like \"max_new_token\" when invoking the llm\n",
    "llm.invoke(prompt=\"tell me a joke\", max_new_tokens=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwKprHnfx0xe",
    "outputId": "4283b411-4f01-4e0f-d55d-e88ecd1369a4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n",
      "WARNING! task is not default parameter.\n",
      "                    task was transferred to model_kwargs.\n",
      "                    Please confirm that task is what you intended.\n",
      "WARNING! huggingfacehub_api_token is not default parameter.\n",
      "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
      "                    Please confirm that huggingfacehub_api_token is what you intended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4204b065c97844f28acec1680f60dd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27b94d495df49aaa84216a45a1d6859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b5e08b3024428ad6e9e6ad0c8bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ec4fc8762a48e488ca5e7636132546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444e414a72e84b7e89e997615f4f2bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_model = llm_utils.ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NPzL1wZ--wN-",
    "outputId": "8ad5c157-c2aa-45be-f181-5d9457057eac",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HuggingFaceH4/zephyr-7b-beta'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ZV6nyT08-nhU",
    "outputId": "bc266244-2e5f-4a63-d026-a87b62b96850",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|>\\nYou're a helpful assistant</s>\\n<|user|>\\nWhat happens when an unstoppable force meets an immovable object?</s>\\n<|assistant|>\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNRZ0FL8_K2x",
    "outputId": "648ebc9b-9bf5-46e8-8543-bd4cbcfa7c27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the popular philosophical concept, when an unstoppable force meets an immovable object, there is a paradox because both concepts seem to contradict each other. The idea of an unstoppable force implies that it can overcome any obstacle, while the concept of an immovable object suggests that it cannot be moved. This paradox raises questions about the nature of force and object, and whether such concepts are absolute or relative. Some interpretations suggest that the force may change or adapt in response to the object, while others suggest that the object may yield or give way under the force's pressure. However, the concept remains a thought-provoking paradox that challenges our understanding of physical laws and the limits of our knowledge.\n"
     ]
    }
   ],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "QP3jiJPtnx70",
    "outputId": "2a7859d2-d860-4b6b-def1-dc9d11c1adf5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lower_case: lower_case(input: str) -> str - Returns the input as all lower case.\\nmultiply: multiply(first_int: int, second_int: int) -> int - Multiply two integers together.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "\n",
    "@tool(\"lower_case\", return_direct=True)\n",
    "def to_lower_case(input:str) -> str:\n",
    "  \"\"\"Returns the input as all lower case.\"\"\"\n",
    "  return input.lower()\n",
    "\n",
    "my_tools = [to_lower_case, multiply]\n",
    "render_text_description(my_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SS4aXzVAaEzA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, load_tools\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import (\n",
    "    ReActJsonSingleInputOutputParser,\n",
    ")\n",
    "prompt = hub.pull(\"hwchase17/react-json\")\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description(my_tools),\n",
    "    tool_names=\", \".join([t.name for t in my_tools]),\n",
    ")\n",
    "\n",
    "# define the agent\n",
    "chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B0dKTLib6bGm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create agent executor with tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=my_tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEPAcyDs7IR1",
    "outputId": "1d18ae3f-8794-4842-a58f-89216f2dbb18",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHere's the JSON blob to calculate the product of 5 and 6:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"action\": \"multiply\",\n",
      "  \"action_input\": {\n",
      "    \"first_int\": 5,\n",
      "    \"second_int\": 6\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Observation\u001b[0m\u001b[33;1m\u001b[1;3m30\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: The product of 5 and 6 is 30.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 5 multiplied by 6?',\n",
       " 'output': 'The product of 5 and 6 is 30.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what is 5 multiplied by 6?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhoUvGblMKNx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
