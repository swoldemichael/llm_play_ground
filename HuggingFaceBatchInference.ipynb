{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzsVu2uUti5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "_Ro37xCHdt0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOKm8vtYuAx",
        "outputId": "f973ca3c-5e79-4e69-8931-6c651c97efa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Phi3ForCausalLM(\n",
              "  (model): Phi3Model(\n",
              "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x Phi3DecoderLayer(\n",
              "        (self_attn): Phi3Attention(\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
              "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Phi3MLP(\n",
              "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (activation_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Phi3RMSNorm()\n",
              "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_attention_layernorm): Phi3RMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): Phi3RMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "P4HiWrRnlkPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_data = [\"The world would be a better place if we all\", \"I am hungry and I want to eat some good\"]"
      ],
      "metadata": {
        "id": "GAWT_IMyciYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "3660yqFpWg1G",
        "outputId": "e6ba9104-a68e-4570-b4bb-7826a64ad906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(batch_data, return_tensors=\"pt\", padding=True).to(device)\n",
        "model_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txuVt3ONcQQe",
        "outputId": "28fe7293-9928-4694-fcba-1b24fffb6e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[32000,   450,  3186,   723,   367,   263,  2253,  2058,   565,   591,\n",
              "           599],\n",
              "        [  306,   626,  9074, 14793,   322,   306,   864,   304, 17545,   777,\n",
              "          1781]], device='cuda:0'), 'attention_mask': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  model_outputs = model.generate(**model_inputs, max_new_tokens=30, do_sample=True, num_beams=4, num_return_sequences=2)"
      ],
      "metadata": {
        "id": "4HM_hLr8WdVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = tokenizer.batch_decode(model_outputs, skip_special_tokens=True)\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuyRjUoOhicO",
        "outputId": "d95e163b-5413-470a-f492-794b38c83410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The world would be a better place if we all had a little more compassion and understanding.\\n\\nSarah: I couldn't agree more. It's important to remember that\",\n",
              " 'The world would be a better place if we all had a little more compassion and understanding.\\n\\nTheory of Mind Exercises:\\n\\n1. What would happen if the',\n",
              " 'I am hungry and I want to eat some good food.\\n\\n# Answer\\nI am hungry and I want to eat some delicious and nutritious food to satisfy my appetite',\n",
              " 'I am hungry and I want to eat some good food.\\n\\n# Answer\\nI am hungry and I would like to have a delicious meal to satisfy my appetite.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}